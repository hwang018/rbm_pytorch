{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid , save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Part 1 : Creating the architecture of the Neural Network \n",
    "\n",
    "# Create the class RBM \n",
    "class RBM(): \n",
    "    # self is the object \n",
    "    # all the variables attached to the object will be created with self. \n",
    "    # nv number of visible nodes \n",
    "    # nh number of hidden nodes \n",
    "    def __init__(self, nv, nh): \n",
    "        #initialize the parameters we optimize during the training weights and bias\n",
    "        #weights used for the probability of the visible nodes given the hidden nodes (p_v_given_h))\n",
    "        # torch.rand : random normal distribution mean=0, variance=1 \n",
    "        self.W = torch.randn(nh,nv)\n",
    "        # bias probability of the hidden nodes given the visible nodes (p_h_given_v))\n",
    "        # fake dimension for the batch = 1\n",
    "        self.a = torch.randn(1,nh)\n",
    "        # bias probability of the visible nodes is activated \n",
    "        #given the value of the hidden nodes (p_v_given_h))\n",
    "        self.b = torch.randn(1, nv)\n",
    "        # we can add more, like learning rate... in this section to be used in train()\n",
    "\n",
    "    def sample_h(self, x): \n",
    "        # probability h is activated given the value v is the sigmoid(Wx+a).\n",
    "        # torch.mm make the product of two tensors. \n",
    "        # W.t()take the transpose because W is used for the p_v_given_h.\n",
    "        wx=torch.mm(x,self.W.t())\n",
    "        # .expand_as(wx) : expand the mini-batch.\n",
    "        activation=wx+self.a.expand_as(wx)\n",
    "        # probability p_h_given_v is the probability that the note drama genre is activated. \n",
    "        # v value is the input value. If v is a film drama, p_h_given_v will be hight. \n",
    "        # If v is not a film drama, p_h_given_v will be low.\n",
    "        p_h_given_v=torch.sigmoid(activation)\n",
    "        # Bernouilli RBM. we predict the user loves the movie or not (0 or 1).\n",
    "        # activation or not activation of the nh neurons. \n",
    "        return p_h_given_v, torch.bernouilli(p_h_given_v)\n",
    "\n",
    "    def sample_v(self, y): \n",
    "        # probability h is activated given the value v is the sigmoid(Wx+a).\n",
    "        # torch.mm make the product of two tensors. \n",
    "        wy=torch.mm(y,self.W)\n",
    "        # .expand_as(wx) : expand the mini-batch.\n",
    "        activation=wy+self.b.expand_as(wy)\n",
    "        p_v_given_h=torch.sigmoid(activation)\n",
    "        # Bernouilli RBM. we predict the user loves the movie or not (0 or 1).\n",
    "        # activation or not activation of the nv neurons. \n",
    "        return p_v_given_h, torch.bernouilli(p_v_given_h)\n",
    "\n",
    "        # Contrastive divergence Algorithm\n",
    "        # Optimize the weights to minimize the energy.\n",
    "        # ~ Maximize the Log-Likelihood of the model. \n",
    "        # Need to approximate the gradients with the algorithm contrastive divergence. \n",
    "    def train(self,v0,vk,ph0,phk):\n",
    "        #ph0,phk regarding to 1 user, this train function\n",
    "        # vk: visible nodes after k round trips of sampling\n",
    "        # ph: ph0: vector prob at first iteration of hidden nodes = 1, given v0\n",
    "        # phk vector prob at k iteration, for h = 1 given vk\n",
    "        \n",
    "        self.W += torch.mm(v0.t(),ph0)-torch.mm(vk.t(),phk)\n",
    "        # add ,0 for the tensor of two dimension \n",
    "        self.b += torch.sum((v0-vk),0) #keep format of v as 2d dim\n",
    "        self.a += torch.sum(ph0-phk,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(torch.FloatTensor([7]),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Part 2 : Create the RBM Object \n",
    "# number of movies\n",
    "nv=len(training_set[0]) \n",
    "# parameter is tunable is the number of features that we want to detect \n",
    "#hidden nodes rep some features rbm it to learn\n",
    "# features ~ genre, actors, director, oscar, date.... \n",
    "nh=100 #to tune\n",
    "# update the weights after serveral observations, also tunable\n",
    "batch_size=100 # each batch train how many samples\n",
    "# Creation of the object of the class RBM()\n",
    "rbm=RBM(nv,nh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nb_users' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-10457187969e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m#taking a batch of users\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mid_user\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnb_users\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#range(a,b,c)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;31m# at the beginning v0=vk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# vk is going to be updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nb_users' is not defined"
     ]
    }
   ],
   "source": [
    "### Part 3 : Training the RBM \n",
    "nb_epoch = 10 \n",
    "# upper bound is no included nb_epoch+1 \n",
    "\n",
    "# First for loop : epoch for loop \n",
    "for epoch in range (1,nb_epoch+1):\n",
    "    #loss function initialized to 0 at the beginning of the trainning \n",
    "    train_loss = 0\n",
    "    # counter which is a float . \n",
    "    s = 0.\n",
    "\n",
    "    # Second for loop : user forloop \n",
    "    # 0 lower bound \n",
    "    # nb_users-batch_size upper bound \n",
    "    # batch_size is the step of each batch (100)\n",
    "    # First batch is from user id=0 ti user id =99\n",
    "    \n",
    "    #taking a batch of users\n",
    "    for id_user in range(0,nb_users-batch_size,batch_size): #range(a,b,c)\n",
    "        # at the beginning v0=vk \n",
    "        # vk is going to be updated\n",
    "        # id_user,id_user+batch_size ~id_user+100\n",
    "        \n",
    "        vk=training_set[id_user:id_user+batch_size] #output of gibbs sampling, now dealing with specific user\n",
    "        v0=training_set[id_user:id_user+batch_size]\n",
    "        #initial probability prob hidden node at start = 1 given original ratings \n",
    "        ph0, _ = rbm.sample_h(v0)\n",
    "        \n",
    "        \n",
    "        # k steps in the random walk\n",
    "        # Third for loop : Contrastive divergence\n",
    "        for k in range(10): #why 10? mcmc technique\n",
    "            #call sample_h on visible nodes, get the first sampling of the first hidden node\n",
    "            _,hk=rbm.sample_h(vk) #v0 is the target, will not change, we take vk\n",
    "            #next we use the obtained hk to get the sampled vk\n",
    "            _,vk=rbm.sample_v(hk) #update vk, second sample of the visible nodes, until all the k steps are sampled and updated vk\n",
    "            \n",
    "            #vk are visible nodes in the kstep random walk\n",
    "            \n",
    "            #above are the actual sampled values \n",
    "            \n",
    "            # we don't want to learn where is no rating by the user\n",
    "            # no update when -1 rating.\n",
    "            vk[v0<0]=v0[v0<0] #training are not done on these -1 ratings\n",
    "            \n",
    "            phk,_=rbm.sample_h(vk) #get the probability of hk, last sample of the visible nodes in the kstep random walk, vk is the input, using this to get the phk\n",
    "            \n",
    "            # maximum likelihood to update the parameters, no return type\n",
    "            rbm.train(v0,vk,ph0,phk)\n",
    "            \n",
    "            # Contrastive divergence to approximate the gradient\n",
    "            \n",
    "            # Compare vk updated after the training to v0 the target. \n",
    "            # simple distance in absolute value \n",
    "            # [vO>=0] take only the value with ratings / coherence with vk[v0<0]=[v0<0]\n",
    "            #update the loss value\n",
    "            \n",
    "            # difference between target v0, and the prediction (last sample of the visible node of the contrastive divergence random walk)\n",
    "            # mean absolute difference between truth and predictions\n",
    "            train_loss+=torch.mean(torch.abs(v0[vO>=0]-vk[vO>=0]))\n",
    "            \n",
    "            #rmse version\n",
    "            #train_loss+=np.sqrt(torch.mean((v0[v0>=0] - vk[v0>=0])**2)) # RMSE here\n",
    "            s += 1.\n",
    "    # in each epoch , see overall loss function        \n",
    "    print('epoch: ' +str(epoch) +' loss: '+str(train_loss/s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making prediction on the test set\n",
    "# testing RBM\n",
    "# mcmc\n",
    "\n",
    "test_loss = 0\n",
    "# counter which is a float . \n",
    "s = 0. #each step increment by 1\n",
    "\n",
    "#taking a batch of users, no need batch in testing\n",
    "for id_user in range(nb_users):\n",
    "    # v is input, need to keep training set, input used to activate the neurons to activate hidden states\n",
    "    # use training set input ratings to activate the neurons and to make predictions on the testset\n",
    "    v=training_set[id_user]\n",
    "    \n",
    "    #vt is the target, original ratings of testset\n",
    "    vt=test_set[id_user]\n",
    "    \n",
    "    # not consider rating of -1 in testset\n",
    "    if len(vt[vt>=0]) >0:\n",
    "        # then to make prediction\n",
    "        # sample hidden nodes first \n",
    "        _,h = rbm.sample_h(v)\n",
    "        # then use the sampled hidden nodes h to sample visible nodes v\n",
    "        _,v = rbm.sample_v(h)\n",
    "        \n",
    "        test_loss+=torch.mean(torch.abs(vt[vt>=0]-v[v>=0])) # excluding places where there are no rating, for fair comparison\n",
    "        #rmse version\n",
    "        #test_loss += np.sqrt(torch.mean((vt[vt>=0] - v[vt>=0])**2)) # RMSE here\n",
    "        s += 1.\n",
    "    print('test loss:'+str(test_loss/s))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
