{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K4f4JG1gdKqj"
   },
   "source": [
    "## Autoencoder based recommendation model\n",
    "\n",
    "Basic structure for autoencoder based recommendation model, to fill the sparse rating matrix with predicted ratings. When doing evaluation on test set, only the existing user-item pairs will be factored in the loss while the prediction result will be a full matrix with all the ratings, from there we can sort items to user and make top-k recommendations. \n",
    "\n",
    "This notebook includes:\n",
    "1. Data preparation and methods in splitting datasets for recommendation train and test process.\n",
    "2. Structure of autoencoder.\n",
    "3. Search for hyperparameters and simple experiments on model structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_LvGeU1CeCtg"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'encoder' from '../models/encoder.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "import sys\n",
    "#insert search path for packages\n",
    "sys.path.insert(0, '../common/')\n",
    "sys.path.insert(0, '../models/')\n",
    "from importlib import reload\n",
    "\n",
    "#project scripts\n",
    "import util as util\n",
    "import preprocessor as prep\n",
    "import encoder as EN\n",
    "reload(prep)\n",
    "reload(util)\n",
    "reload(EN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.read_csv('../dataset/ml-1m/ratings.dat', delimiter = '::',header=None, engine='python')\n",
    "ratings.columns = ['userid','itemid','rating','timestamp']\n",
    "ratings_reindex = prep.reindexer(ratings,'userid','itemid','rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(ratings_reindex,\n",
    "                               stratify=ratings_reindex['encoded_users'],\n",
    "                               test_size=0.1,\n",
    "                               random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-wASs2YFiDaa"
   },
   "outputs": [],
   "source": [
    "# creating matrix form of ratings\n",
    "def convert(data):\n",
    "    new_data = []\n",
    "    for id_users in range(nb_users+1):\n",
    "        # each user's watched movies\n",
    "        # data[:,0], first column, all rows column users\n",
    "        id_items = data[:,1][data[:,0] == id_users]\n",
    "        # each user's rating for that item\n",
    "        id_ratings = data[:,2][data[:,0] == id_users]\n",
    "        ratings = np.zeros(nb_movies)\n",
    "        # the positions of these items are filled with ratings, creating the matrix\n",
    "        ratings[id_items-1] = id_ratings\n",
    "        new_data.append(list(ratings))\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.9 s, sys: 891 ms, total: 21.8 s\n",
      "Wall time: 21.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "full_records = np.array(ratings_reindex, dtype = 'int')\n",
    "full_records = convert(full_records)\n",
    "full_records = torch.FloatTensor(full_records)\n",
    "\n",
    "training_set = np.array(train, dtype = 'int')\n",
    "test_set = np.array(test, dtype = 'int')\n",
    "\n",
    "nb_users = int(max(max(training_set[:,0]), max(test_set[:,0])))\n",
    "nb_movies = int(max(max(training_set[:,1]), max(test_set[:,1])))\n",
    "\n",
    "\n",
    "training_set = convert(training_set)\n",
    "training_set = torch.FloatTensor(training_set)\n",
    "\n",
    "test_set = convert(test_set)\n",
    "test_set = torch.FloatTensor(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6kkL8NkkdlZj"
   },
   "source": [
    "### 2.Automated creation of encoder structure\n",
    "\n",
    "Create autoencoder network structure based on a list\n",
    "containing the units\n",
    "\n",
    "e.g. \n",
    "[input_size,20,10] means there are 2 + 2 layers in total\n",
    "(len(L)-1 * 2) number of layers in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder_network = EN.Encoder([nb_movies,20,10],'sigmoid',0.1)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.RMSprop(autoencoder_network.parameters(), lr = 0.01, weight_decay = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7gy59alAdloL"
   },
   "source": [
    "### 3.Training encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "FEz9hRaciFTs",
    "outputId": "0f6ed0d0-09c4-46c0-bfe6-70031d76b491"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 loss: tensor(1.2802)\n",
      "epoch: 2 loss: tensor(0.9990)\n",
      "epoch: 3 loss: tensor(0.9854)\n",
      "epoch: 4 loss: tensor(0.9803)\n",
      "epoch: 5 loss: tensor(0.9785)\n",
      "epoch: 6 loss: tensor(0.9769)\n",
      "epoch: 7 loss: tensor(0.9764)\n",
      "epoch: 8 loss: tensor(0.9757)\n",
      "epoch: 9 loss: tensor(0.9756)\n",
      "epoch: 10 loss: tensor(0.9752)\n"
     ]
    }
   ],
   "source": [
    "nb_epoch = 10\n",
    "for epoch in range(1, nb_epoch + 1):\n",
    "    train_loss = 0\n",
    "    s = 0.\n",
    "    # s is the number of users who rated at least 1 movies\n",
    "    for id_user in range(nb_users):\n",
    "        input = Variable(training_set[id_user]).unsqueeze(0)\n",
    "        target = input.clone()\n",
    "        if torch.sum(target.data > 0) > 0:\n",
    "            output = autoencoder_network(input)\n",
    "            target.require_grad = False\n",
    "            output[target == 0] = 0\n",
    "            loss = criterion(output, target)\n",
    "            mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10) #making this anyway not equal to 0, as this will be a denominator\n",
    "            #mean_corrector is the avg of the error, only considering the movies having ratings (non-zero ratings) for computing mean of error\n",
    "            loss.backward() # decide the direction the increment of weights\n",
    "            #this call will just computing all the gradients required\n",
    "            train_loss += np.sqrt(loss.data*mean_corrector)\n",
    "            s += 1.\n",
    "            optimizer.step() # decide the amount to update the weights\n",
    "            \n",
    "    print('epoch: '+str(epoch)+' loss: '+ str(train_loss/s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bak5uc8gd-gX"
   },
   "source": [
    "### 4.Testing encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5ztvzYRtiGCz",
    "outputId": "d0e8ea8b-9ac4-40e5-a19a-7fcfc6934d61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: tensor(0.9523)\n"
     ]
    }
   ],
   "source": [
    "test_loss = 0\n",
    "s = 0.\n",
    "\n",
    "res = []\n",
    "targets = []\n",
    "\n",
    "# averaged difference between real rating and predicted rating\n",
    "for id_user in range(nb_users):\n",
    "    input = Variable(training_set[id_user]).unsqueeze(0) # should keep the training set\n",
    "    target = Variable(test_set[id_user]).unsqueeze(0) # to predict the other movies user not seen yet\n",
    "    \n",
    "    if torch.sum(target.data > 0) > 0:\n",
    "        # make predictions\n",
    "        output = autoencoder_network(input)\n",
    "        targets.append(target.detach().numpy())\n",
    "        res.append(output.detach().numpy()) \n",
    "        target.require_grad = False\n",
    "        output[target == 0] = 0 # dont want to measure the loss on the movies didnt get the actual rating from user \n",
    "        # force to 0 and difference / loss will be 0 for those entries\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10) \n",
    "        # only consider the movies that are rated in the test set, to be included in the loss\n",
    "        test_loss += np.sqrt(loss.data*mean_corrector)\n",
    "        s += 1.\n",
    "print('test loss: '+str(test_loss/s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.Recommend top K items, should have choice of including or not the already seen movies\n",
    "\n",
    "This is for the purpose of getting the general use case of selling the same product multiple times.\n",
    "We use the full records as the input to the encoder, the autoencoder will predict for each user all the items' rating, we can also choose to filter out the already seen items from the dense matrix output.\n",
    "\n",
    "From the dense matrix, we can already sort the predicted ratings and make top-k items recommendation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_top_k_recommendations(encoder,evidence,k,filter_seen=True):\n",
    "    '''\n",
    "    :param encoder: autoencoder instance\n",
    "    :param evidence: full set of seen ratings from all users\n",
    "    :param k: top k items (by output score)\n",
    "    :param filter_seen: (default True) filter controller to remove seen items from top k list\n",
    "    '''     \n",
    "    res = []\n",
    "    nb_users = evidence.shape[0]\n",
    "    # to find top scored items for each user\n",
    "    for id_user in range(nb_users):\n",
    "        encoder_input = Variable(evidence[id_user]).unsqueeze(0) # should keep the training set \n",
    "        encoder_output = encoder(encoder_input)\n",
    "        \n",
    "        target = Variable(evidence[id_user]).unsqueeze(0) # mask to find items not seen yet\n",
    "        if filter_seen:\n",
    "            encoder_output[target != 0] = 0 # force seen items scores to 0, will never get recommended\n",
    "        res.append(encoder_output.detach().numpy())\n",
    "        \n",
    "    res = [a[0] for a in res]\n",
    "    final_itemsets = []    \n",
    "    for each in res:\n",
    "        full_ratings_predicted = list(each)\n",
    "        full_ratings_indexed = list(enumerate(full_ratings_predicted))\n",
    "        final_itemsets.append(sorted(full_ratings_indexed,key=lambda x:x[1],reverse =True)[:k])\n",
    "        \n",
    "    return final_itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = make_top_k_recommendations(autoencoder_network,full_records,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({513: 3737,\n",
       "         1133: 2058,\n",
       "         3203: 187,\n",
       "         708: 3,\n",
       "         1839: 27,\n",
       "         2698: 11,\n",
       "         2785: 6,\n",
       "         309: 1,\n",
       "         2748: 1,\n",
       "         1117: 8,\n",
       "         1104: 1,\n",
       "         1848: 1})"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "# distribution of top 1 item among all users\n",
    "Counter([a[0][0] for a in A])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "AutoEncoders.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
